{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jj1tSuKVd_jt"
      },
      "outputs": [],
      "source": [
        "!pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAw5cZ8sMXIo"
      },
      "outputs": [],
      "source": [
        "!pip install --no-deps xformers trl peft accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sl1jjxFMeVx"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from huggingface_hub import notebook_login\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import FastLanguageModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mYT-2s2d_ju"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQQN-uSUzB8h"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"hugging_face_username\":\"Apurva3509\",\n",
        "    \"model_config\": {\n",
        "        \"base_model\":\"unsloth/gemma-1.1-7b-it-bnb-4bit\",\n",
        "        \"finetuned_model\":\"gemma-1.1-7b-it-bnb-4bit-medical\",\n",
        "        \"max_seq_length\": 2048,\n",
        "        \"dtype\":torch.float16,\n",
        "        \"load_in_4bit\": True, ]\n",
        "    },\n",
        "    \"lora_config\": {\n",
        "      \"r\": 16,\n",
        "      \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "      \"lora_alpha\":16,\n",
        "      \"lora_dropout\":0,\n",
        "      \"bias\":\"none\",\n",
        "      \"use_gradient_checkpointing\":True,\n",
        "      \"use_rslora\":False,\n",
        "      \"use_dora\":False,\n",
        "      \"loftq_config\":None\n",
        "    },\n",
        "    \"training_dataset\":{\n",
        "        \"name\":\"data/medical_gemma_instruct_dataset_short\",\n",
        "        \"split\":\"train\",\n",
        "        \"input_field\":\"prompt\",\n",
        "    },\n",
        "    \"training_config\": {\n",
        "        \"per_device_train_batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"warmup_steps\": 5,\n",
        "        \"max_steps\":0,\n",
        "        \"num_train_epochs\": 1,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"fp16\": not torch.cuda.is_bf16_supported(),\n",
        "        \"bf16\": torch.cuda.is_bf16_supported(),\n",
        "        \"logging_steps\": 1,\n",
        "        \"optim\" :\"adamw_8bit\",\n",
        "        \"weight_decay\" : 0.01,\n",
        "        \"lr_scheduler_type\": \"linear\",\n",
        "        \"seed\" : 42,\n",
        "        \"output_dir\" : \"outputs\",\n",
        "    }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4wxJAgnM2W0"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = config.get(\"model_config\").get(\"base_model\"),\n",
        "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "    dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "    load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = config.get(\"lora_config\").get(\"r\"),\n",
        "    target_modules = config.get(\"lora_config\").get(\"target_modules\"),\n",
        "    lora_alpha = config.get(\"lora_config\").get(\"lora_alpha\"),\n",
        "    lora_dropout = config.get(\"lora_config\").get(\"lora_dropout\"),\n",
        "    bias = config.get(\"lora_config\").get(\"bias\"),\n",
        "    use_gradient_checkpointing = config.get(\"lora_config\").get(\"use_gradient_checkpointing\"),\n",
        "    random_state = 42,\n",
        "    use_rslora = config.get(\"lora_config\").get(\"use_rslora\"),\n",
        "    use_dora = config.get(\"lora_config\").get(\"use_dora\"),\n",
        "    loftq_config = config.get(\"lora_config\").get(\"loftq_config\"),\n",
        ")\n",
        "\n",
        "dataset_train = load_dataset(config.get(\"training_dataset\").get(\"name\"), split = config.get(\"training_dataset\").get(\"split\"))\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset_train,\n",
        "    dataset_text_field = config.get(\"training_dataset\").get(\"input_field\"),\n",
        "    max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = config.get(\"training_config\").get(\"per_device_train_batch_size\"),\n",
        "        gradient_accumulation_steps = config.get(\"training_config\").get(\"gradient_accumulation_steps\"),\n",
        "        warmup_steps = config.get(\"training_config\").get(\"warmup_steps\"),\n",
        "        max_steps = config.get(\"training_config\").get(\"max_steps\"),\n",
        "        num_train_epochs= config.get(\"training_config\").get(\"num_train_epochs\"),\n",
        "        learning_rate = config.get(\"training_config\").get(\"learning_rate\"),\n",
        "        fp16 = config.get(\"training_config\").get(\"fp16\"),\n",
        "        bf16 = config.get(\"training_config\").get(\"bf16\"),\n",
        "        logging_steps = config.get(\"training_config\").get(\"logging_steps\"),\n",
        "        optim = config.get(\"training_config\").get(\"optim\"),\n",
        "        weight_decay = config.get(\"training_config\").get(\"weight_decay\"),\n",
        "        lr_scheduler_type = config.get(\"training_config\").get(\"lr_scheduler_type\"),\n",
        "        seed = 42,\n",
        "        output_dir = config.get(\"training_config\").get(\"output_dir\"),\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XNnVYIk6d_jv"
      },
      "outputs": [],
      "source": [
        "gpu_statistics = torch.cuda.get_device_properties(0)\n",
        "reserved_memory = round(torch.cuda.max_memory_reserved() / 1024**3, 2)\n",
        "max_memory = round(gpu_statistics.total_memory / 1024**3, 2)\n",
        "print(f\"Reserved Memory: {reserved_memory}GB\")\n",
        "print(f\"Max Memory: {max_memory}GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yI9mEQ7ZOUx2"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2yI5EBNNd_jv"
      },
      "outputs": [],
      "source": [
        "used_memory = round(torch.cuda.max_memory_allocated() / 1024**3, 2)\n",
        "used_memory_lora = round(used_memory - reserved_memory, 2)\n",
        "used_memory_persentage = round((used_memory / max_memory) * 100, 2)\n",
        "used_memory_lora_persentage = round((used_memory_lora / max_memory) * 100, 2)\n",
        "print(f\"Used Memory: {used_memory}GB ({used_memory_persentage}%)\")\n",
        "print(f\"Used Memory for training(fine-tuning) LoRA: {used_memory_lora}GB ({used_memory_lora_persentage}%)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wTgSXl7Ud_jw"
      },
      "outputs": [],
      "source": [
        "with open(\"trainer_stats.json\", \"w\") as f:\n",
        "    json.dump(trainer_stats, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1HtsRpVnHTj"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(config.get(\"model_config\").get(\"finetuned_model\"))\n",
        "model.push_to_hub(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer = tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AO3A3J0Ud_jw"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_16bit\",)\n",
        "model.push_to_hub_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_16bit\")\n",
        "\n",
        "model.save_pretrained_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_4bit\",)\n",
        "model.push_to_hub_merged(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, save_method = \"merged_4bit\")\n",
        "\n",
        "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer)\n",
        "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer)\n",
        "\n",
        "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"f16\")\n",
        "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"f16\")\n",
        "\n",
        "model.save_pretrained_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")\n",
        "model.push_to_hub_gguf(config.get(\"model_config\").get(\"finetuned_model\"), tokenizer, quantization_method = \"q4_k_m\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ozVcalyP_JLs"
      },
      "outputs": [],
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = config.get(\"model_config\").get(\"finetuned_model\"),\n",
        "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "        load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        "    )\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    \"<start_of_turn>user Answer the question truthfully, you are a medical professional. This is the question: Can you provide an overview of the lung's squamous cell carcinoma?<end_of_turn>\"\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "tokenizer.decode(outputs, skip_special_tokens = True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
