# MedLMs
 Medical grade LLM finetuning

![Project Banner](image2.png)

This repository contains the code, results, and insights from analyzing the performance of advanced language models for medical domain tasks. Models such as **Llama 3-8B**, **Gemma 1.1-7B**, and **DistilGPT2** are fine-tuned and evaluated based on BLEU, ROUGE, and accuracy metrics across multiple configurations.

---

## Key Highlights

### Key Features:
- Comprehensive **BLEU**, **ROUGE**, and **Accuracy** comparison for models.
- Analysis of **hardware configurations** (RTX8000, A100, V100) for inference efficiency.
- Evaluation of **model architecture** impact on task-specific performance.

### Key Findings:
- Larger models like **Llama 3-8B** outperform smaller models due to better architecture and pre-training.
- **RTX8000** shows optimal inference performance, while **A100** exhibits unexpected delays.
- Rapid learning during early epochs (5â€“7) with saturation at epoch 15.

---

## Setup and Usage

### ðŸ›  Requirements
- Python >= 3.8
- PyTorch >= 1.9.0
- Hugging Face Transformers Library
- CUDA-enabled GPUs

### ðŸ“¦ Installation
Clone this repository and install the required packages:
```bash
git clone https://github.com/Apurva3509/medical-model-performance.git
cd Files
```

### ðŸš€ Running the Code
To train and evaluate models, deploy them from hugging face and play with the parameters to see the responses for differnet inputs.

---


### Tables
----------------------------------------------------------------------------------------
| Model           | BLEU Score | ROUGE Score | Accuracy (%) | Avg. Inference Time (s)  |
|-----------------|------------|-------------|--------------|--------------------------|
| **Llama 3-8B**  | 0.59       | 0.58        | 78.5         | 9.5                      |
| **Gemma 1.1-7B**| 0.54       | 0.52        | 74.0         | 10.8                     |
| **DistilGPT2**  | 0.31       | 0.29        | 50.2         | 16.5                     |
----------------------------------------------------------------------------------------

## Project Structure
```
MedLMs/
â”œâ”€â”€ Files/
â”‚   â”œâ”€â”€ .DS_Store
â”‚   â”œâ”€â”€ DistilGPT_finetuning.ipynb
â”‚   â”œâ”€â”€ DistilGPT_finetuning_v1.2.ipynb
â”‚   â”œâ”€â”€ Distilgpt2_vs_Llama3.ipynb
â”‚   â”œâ”€â”€ Evaluating_finetuned-distill-vs-gemma.ipynb
â”‚   â”œâ”€â”€ Evaluating_finetuned_LLMs-distilgpt2.ipynb
â”‚   â”œâ”€â”€ Evaluating_finetuned_LLMs-v1.0.ipynb
â”‚   â”œâ”€â”€ Llama_2_7b_MedLM.ipynb
â”‚   â”œâ”€â”€ Mistral_7b_MedLM.ipynb
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ gemma_1.1_7b_it_medical.ipynb
â”‚   â”œâ”€â”€ llama_3_8b_instruct_medical-new.ipynb
â”‚   â”œâ”€â”€ medical_models_evaluation.csv
â”‚   â””â”€â”€ results-images/
â”‚       â”œâ”€â”€ .DS_Store
â”‚       â”œâ”€â”€ Accuracy vs Number of Epochs.png
â”‚       â”œâ”€â”€ BLEU Scores vs Number of Epochs.png
â”‚       â”œâ”€â”€ ROUGE Scores vs Number of Epochs.png
â”‚       â”œâ”€â”€ W&B Chart 11_30_2024, 10_56_31 PM.png
â”‚       â”œâ”€â”€ W&B Chart 11_30_2024, 10_56_31 PM.svg
â”‚       â”œâ”€â”€ gpu_inference_comparison.html
â”‚       â”œâ”€â”€ newplot-v1.png
â”‚       â””â”€â”€ newplot-v2.png
â”œâ”€â”€ Presentation/
â”‚   â”œâ”€â”€ .DS_Store
â”‚   â”œâ”€â”€ EECS E6694 Final Presentation.pdf
â”‚   â”œâ”€â”€ EECS E6694 Final Presentation.pptx
â”œâ”€â”€ Report/
â”‚   â”œâ”€â”€ E6694 GenAI Report.zip
â”‚   â”œâ”€â”€ E6694_GenAI_Report.pdf
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ all_models.png
â”‚   â”œâ”€â”€ gemma_loss.csv
â”‚   â”œâ”€â”€ gemma_loss.png
â”‚   â”œâ”€â”€ llama2_loss.csv
â”‚   â”œâ”€â”€ llama2_loss.png
â”‚   â”œâ”€â”€ llama3_loss.csv
â”‚   â”œâ”€â”€ llama3_loss.png
â”‚   â”œâ”€â”€ mistral_loss.csv
â”‚   â”œâ”€â”€ mistral_loss.png
â”‚   â”œâ”€â”€ trainer_stats_gemma.json
â”‚   â”œâ”€â”€ trainer_stats_llama2.json
â”‚   â”œâ”€â”€ trainer_stats_llama3.json
â”‚   â”œâ”€â”€ trainer_stats_mistral.json
â”œâ”€â”€ Data/
â”‚   â”œâ”€â”€ instruct_datasets.py
â”‚   â”œâ”€â”€ medical_meadow_wikidoc.csv
â”‚   â”œâ”€â”€ medquad.csv
â”œâ”€â”€ .DS_Store            
â”œâ”€â”€ .gitattributes      
â”œâ”€â”€ .gitignore           
â”œâ”€â”€ LICENSE             
â”œâ”€â”€ README.md            
â”œâ”€â”€ image.png            
â”œâ”€â”€ image2.png           
â”œâ”€â”€ requirements.txt
```

---

## Contributing
We welcome contributions! To contribute:
1. Fork the repository.
2. Create a feature branch (`git checkout -b feature-name`).
3. Submit a pull request.

---

## License
This project is licensed under the MIT License. See the [LICENSE](LICENSE) file for details.

---

## Acknowledgements
Special thanks to:
- **Hugging Face** for providing pre-trained models.
- **NVIDIA** for hardware configurations used in experiments.

For detailed results and models, check our Hugging Face repository:
[Apurva](https://huggingface.co/Apurva3509)
[Abhilash](https://huggingface.co/abhilash2599)
