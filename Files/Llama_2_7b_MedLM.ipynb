{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjTpvR11uecN"
      },
      "source": [
        "### Installing Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0jHc7fluF8J"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install --no-deps packaging ninja einops flash-attn xformers trl peft accelerate bitsandbytes\n",
        "!pip install evaluate\n",
        "!pip install rouge-score\n",
        "!pip install bert-score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nqRUPN2Xva_p"
      },
      "source": [
        "### Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JwGH8n3iuP-f"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# Define configuration for the base model, LoRA, and training\n",
        "config = {\n",
        "    \"model_config\": {\n",
        "        \"base_model\": \"unsloth/llama-2-7b-chat-bnb-4bit\",\n",
        "        \"finetuned_model\": \"abhilash2599/llama-2-7b-medlm-2k\",\n",
        "        \"max_seq_length\": 2048,\n",
        "        \"dtype\": torch.float16,\n",
        "        \"load_in_4bit\": True,\n",
        "    },\n",
        "    \"lora_config\": {\n",
        "          \"r\": 16,\n",
        "          \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                          \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "          \"lora_alpha\":16,\n",
        "          \"lora_dropout\":0.25,\n",
        "          \"bias\":\"none\",\n",
        "          \"use_gradient_checkpointing\":True,\n",
        "          \"use_rslora\":False,\n",
        "          \"use_dora\":False,\n",
        "          \"loftq_config\":None\n",
        "        },\n",
        "    \"training_dataset\": {\n",
        "        \"name\": \"Shekswess/medical_llama2_instruct_dataset_short\",\n",
        "        \"split\": \"train\",\n",
        "        \"input_field\": \"prompt\",\n",
        "    },\n",
        "    \"training_config\": {\n",
        "        \"per_device_train_batch_size\": 2,\n",
        "        \"gradient_accumulation_steps\": 4,\n",
        "        \"warmup_steps\": 5,\n",
        "        \"max_steps\": 0,\n",
        "        \"num_train_epochs\": 1,\n",
        "        \"learning_rate\": 2e-4,\n",
        "        \"fp16\": not torch.cuda.is_bf16_supported(),\n",
        "        \"bf16\": torch.cuda.is_bf16_supported(),\n",
        "        \"logging_steps\": 1,\n",
        "        \"optim\": \"adamw_8bit\",\n",
        "        \"weight_decay\": 0.01,\n",
        "        \"lr_scheduler_type\": \"linear\",\n",
        "        \"seed\": 42,\n",
        "        \"output_dir\": \"outputs\",\n",
        "    },\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWsjDhcCvgCC"
      },
      "source": [
        "### Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jk2pKFKJvX6u"
      },
      "outputs": [],
      "source": [
        "# Load and split the dataset\n",
        "def prepare_data(config):\n",
        "    dataset = load_dataset(config[\"training_dataset\"][\"name\"], split=config[\"training_dataset\"][\"split\"])\n",
        "    return dataset.train_test_split(test_size=0.05, seed=config[\"training_config\"][\"seed\"])\n",
        "\n",
        "# Prepare train and test datasets\n",
        "data_splits = prepare_data(config)\n",
        "dataset_train = data_splits[\"train\"]\n",
        "dataset_test = data_splits[\"test\"]\n",
        "\n",
        "print(f\"Training size: {len(dataset_train)}, Test size: {len(dataset_test)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWxr7wrgvvd5"
      },
      "source": [
        "### Model Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNweIU0Pvot4"
      },
      "outputs": [],
      "source": [
        "# Load the model and tokenizer\n",
        "def load_model(config):\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=config[\"model_config\"][\"base_model\"],\n",
        "        max_seq_length=config[\"model_config\"][\"max_seq_length\"],\n",
        "        dtype=config[\"model_config\"][\"dtype\"],\n",
        "        load_in_4bit=config[\"model_config\"][\"load_in_4bit\"],\n",
        "    )\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RI-yEtnv6q0"
      },
      "source": [
        "### Configure LoRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UIuz1VQvxRe"
      },
      "outputs": [],
      "source": [
        "# Setup LoRA for the base model\n",
        "def configure_lora(model, config):\n",
        "    return FastLanguageModel.get_peft_model(\n",
        "        model,\n",
        "        r=config[\"lora_config\"][\"r\"],\n",
        "        target_modules=config[\"lora_config\"][\"target_modules\"],\n",
        "        lora_alpha=config[\"lora_config\"][\"lora_alpha\"],\n",
        "        lora_dropout=config[\"lora_config\"][\"lora_dropout\"],\n",
        "        bias=config[\"lora_config\"][\"bias\"],\n",
        "        use_gradient_checkpointing=config[\"lora_config\"][\"use_gradient_checkpointing\"],\n",
        "        random_state=config[\"training_config\"][\"seed\"],\n",
        "        use_rslora=config[\"lora_config\"][\"use_rslora\"],\n",
        "        use_dora=config[\"lora_config\"][\"use_dora\"],\n",
        "        loftq_config=config[\"lora_config\"][\"loftq_config\"],\n",
        "    )\n",
        "\n",
        "# Apply LoRA configuration to the model\n",
        "model = configure_lora(model, config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FNkO2_GdwWXs"
      },
      "source": [
        "### Training setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmSYNuBmwP5i"
      },
      "outputs": [],
      "source": [
        "# Setup the trainer\n",
        "def setup_trainer(model, tokenizer, train_dataset, config):\n",
        "    return SFTTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train_dataset,\n",
        "        dataset_text_field=config[\"training_dataset\"][\"input_field\"],\n",
        "        max_seq_length=config[\"model_config\"][\"max_seq_length\"],\n",
        "        dataset_num_proc=2,\n",
        "        packing=False,\n",
        "        args=TrainingArguments(\n",
        "            per_device_train_batch_size=config[\"training_config\"][\"per_device_train_batch_size\"],\n",
        "            gradient_accumulation_steps=config[\"training_config\"][\"gradient_accumulation_steps\"],\n",
        "            warmup_steps=config[\"training_config\"][\"warmup_steps\"],\n",
        "            max_steps=config[\"training_config\"][\"max_steps\"],\n",
        "            num_train_epochs=config[\"training_config\"][\"num_train_epochs\"],\n",
        "            learning_rate=config[\"training_config\"][\"learning_rate\"],\n",
        "            fp16=config[\"training_config\"][\"fp16\"],\n",
        "            bf16=config[\"training_config\"][\"bf16\"],\n",
        "            logging_steps=config[\"training_config\"][\"logging_steps\"],\n",
        "            optim=config[\"training_config\"][\"optim\"],\n",
        "            weight_decay=config[\"training_config\"][\"weight_decay\"],\n",
        "            lr_scheduler_type=config[\"training_config\"][\"lr_scheduler_type\"],\n",
        "            seed=config[\"training_config\"][\"seed\"],\n",
        "            output_dir=config[\"training_config\"][\"output_dir\"],\n",
        "        ),\n",
        "    )\n",
        "\n",
        "trainer = setup_trainer(model, tokenizer, dataset_train, config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV3n8FjJwcIe"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sAGsq4bdwYgF"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer_stats = trainer.train()\n",
        "print(\"Training complete. Stats:\", trainer_stats)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hDdDca8PAr0T"
      },
      "outputs": [],
      "source": [
        "# Saving the trainer stats\n",
        "import json\n",
        "with open(\"trainer_stats.json\", \"w\") as f:\n",
        "    json.dump(trainer_stats, f, indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5CsXscCww9T"
      },
      "source": [
        "### Save and Publish Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87_GnsXE8lTM"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "# Logging into the Hugging Face Hub(with token)\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9wk_OGgGCsMy"
      },
      "outputs": [],
      "source": [
        "# Save and push the trained model\n",
        "model.save_pretrained(config.get(\"model_config\").get(\"finetuned_model\"))\n",
        "model.push_to_hub('abhilash2599/llama-2-7b-medlm-2k', tokenizer = tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSYx23wwyxPu"
      },
      "source": [
        "### Sample Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cv333Jv_yzJW"
      },
      "outputs": [],
      "source": [
        "# Loading the fine-tuned model and the tokenizer for inference\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = config.get(\"model_config\").get(\"finetuned_model\"),\n",
        "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "        load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        "    )\n",
        "\n",
        "# Using FastLanguageModel for fast inference\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Tokenizing the input and generating the output\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    \"[INST] Answer the question truthfully, you are a medical professional. This is the question: What is (are) Bloom syndrome ? [/INST]\"\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens = 256, use_cache = True)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens = True))\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "print(f\"Execution time: {end_time - start_time:.2f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_p3dIgRy2mN"
      },
      "source": [
        "### Inference on Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFabyITKzG95"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "import logging\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = config.get(\"model_config\").get(\"finetuned_model\"),\n",
        "        max_seq_length = config.get(\"model_config\").get(\"max_seq_length\"),\n",
        "        dtype = config.get(\"model_config\").get(\"dtype\"),\n",
        "        load_in_4bit = config.get(\"model_config\").get(\"load_in_4bit\"),\n",
        "    )\n",
        "\n",
        "def extract_relevant_input(example):\n",
        "    \"\"\"\n",
        "    Extracts the relevant input for the model from the dataset example in the specified format.\n",
        "    Args:\n",
        "        example (dict): A single data example with 'prompt' and 'output' fields.\n",
        "    Returns:\n",
        "        str: The relevant input for the model.\n",
        "    \"\"\"\n",
        "    prompt = example[\"prompt\"]\n",
        "\n",
        "    # Define the markers for the format\n",
        "    inst_start_marker = \"[INST]\"\n",
        "    inst_end_marker = \"[/INST]\"\n",
        "    response_split_marker = \"\\\\n\"\n",
        "\n",
        "    # Extract the text between [INST] and [/INST]\n",
        "    inst_start = prompt.find(inst_start_marker)\n",
        "    inst_end = prompt.find(inst_end_marker, inst_start)\n",
        "\n",
        "    if inst_start == -1 or inst_end == -1:\n",
        "        raise ValueError(f\"Invalid prompt format: {prompt}\")\n",
        "\n",
        "    # Extract the relevant portion including [INST] and [/INST]\n",
        "    relevant_text = prompt[inst_start:inst_end + len(inst_end_marker)]\n",
        "\n",
        "    # Return the part before the model response starts (split by `\\n`)\n",
        "    if response_split_marker in relevant_text:\n",
        "        relevant_text = relevant_text.split(response_split_marker)[0].strip()\n",
        "\n",
        "    return relevant_text\n",
        "\n",
        "\n",
        "def extract_relevant_text(prediction):\n",
        "    \"\"\"\n",
        "    Extracts the relevant response from the model's prediction.\n",
        "    Assumes the format: [INST] <instruction> [/INST] \\n <response>\n",
        "    Args:\n",
        "        prediction (str): The prediction string from the model.\n",
        "    Returns:\n",
        "        str: The extracted response text.\n",
        "    \"\"\"\n",
        "    marker = \"[/INST] \\\\n\"\n",
        "    if marker in prediction:\n",
        "        return prediction.split(marker, 1)[1].strip()  # Extract and strip whitespace\n",
        "    return prediction.strip()  # Return the full text if the marker is not found\n",
        "\n",
        "def generate_predictions_batch(prompts, batch_size=16):\n",
        "    FastLanguageModel.for_inference(model)  # Ensure the model is in inference mode\n",
        "    all_predictions = []\n",
        "    total_batches = len(prompts) // batch_size + int(len(prompts) % batch_size > 0)\n",
        "\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    for i in range(0, len(prompts), batch_size):\n",
        "        batch_prompts = prompts[i:i + batch_size]\n",
        "        inputs = tokenizer(batch_prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(\"cuda\")\n",
        "\n",
        "        # Adjusting generation parameters to avoid repetition\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=256,         # Limit the output length to avoid long sequences\n",
        "            eos_token_id=eos_token_id,  # Ensure generation stops at EOS token\n",
        "            use_cache=True,\n",
        "            no_repeat_ngram_size=2,    # Avoid repeating n-grams (e.g., repeated phrases)\n",
        "            top_p=0.92,                # Use nucleus sampling for diversity\n",
        "            top_k=50,                  # Limit the pool of potential tokens\n",
        "            temperature=0.7            # Control randomness\n",
        "        )\n",
        "\n",
        "        batch_predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        # Post-process predictions to remove redundant content\n",
        "        batch_predictions_cleaned = [extract_relevant_text(prediction) for prediction in batch_predictions]\n",
        "        all_predictions.extend(batch_predictions_cleaned)\n",
        "\n",
        "        logging.info(f\"Processed batch {i // batch_size + 1}/{total_batches}\")\n",
        "\n",
        "    return all_predictions\n",
        "\n",
        "\n",
        "inputs = [extract_relevant_input(example) for example in dataset_test]\n",
        "references = [example[\"output\"] for example in dataset_test]\n",
        "\n",
        "\n",
        "# Generate predictions in batches\n",
        "predictions = generate_predictions_batch(inputs, batch_size=16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63go-H_pzQ5q"
      },
      "outputs": [],
      "source": [
        "predictions[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Agm48GczS8n"
      },
      "outputs": [],
      "source": [
        "references[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJmhn1MMzYOY"
      },
      "source": [
        "### Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWsqDf6kzX9D"
      },
      "outputs": [],
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "import bert_score\n",
        "\n",
        "def calculate_metrics(predictions, references):\n",
        "    \"\"\"\n",
        "    Calculate Exact Match, Precision, Recall, F1 Score, BLEU, ROUGE, and BERT scores.\n",
        "    Args:\n",
        "        predictions: List of generated predictions.\n",
        "        references: List of ground-truth references.\n",
        "    Returns:\n",
        "        A dictionary containing all metrics with their average and max values.\n",
        "    \"\"\"\n",
        "    # Ensure inputs are tokenized properly\n",
        "    tokenized_predictions = [set(pred.split()) for pred in predictions]\n",
        "    tokenized_references = [set(ref.split()) for ref in references]\n",
        "\n",
        "    # Calculate BLEU\n",
        "    tokenized_refs_for_bleu = [[ref.split()] for ref in references]\n",
        "    tokenized_preds_for_bleu = [pred.split() for pred in predictions]\n",
        "    bleu = corpus_bleu(tokenized_refs_for_bleu, tokenized_preds_for_bleu)\n",
        "\n",
        "    # Calculate ROUGE\n",
        "    rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge_scores = [rouge.score(ref, pred) for ref, pred in zip(references, predictions)]\n",
        "    rouge_averages = {\n",
        "        metric: sum(score[metric].fmeasure for score in rouge_scores) / len(rouge_scores)\n",
        "        for metric in ['rouge1', 'rouge2', 'rougeL']\n",
        "    }\n",
        "    rouge_max = {\n",
        "        metric: max(score[metric].fmeasure for score in rouge_scores)\n",
        "        for metric in ['rouge1', 'rouge2', 'rougeL']\n",
        "    }\n",
        "\n",
        "    # Calculate BERT Score\n",
        "    bert_precision, bert_recall, bert_f1 = bert_score.score(predictions, references, lang=\"en\", verbose=False)\n",
        "    bert_precision_mean = bert_precision.mean().item()\n",
        "    bert_recall_mean = bert_recall.mean().item()\n",
        "    bert_f1_mean = bert_f1.mean().item()\n",
        "    bert_precision_max = bert_precision.max().item()\n",
        "    bert_recall_max = bert_recall.max().item()\n",
        "    bert_f1_max = bert_f1.max().item()\n",
        "\n",
        "    # Calculate Precision, Recall, and F1 at token level (intersection-based)\n",
        "    true_positives = sum(len(pred & ref) for pred, ref in zip(tokenized_predictions, tokenized_references))\n",
        "    predicted_tokens = sum(len(pred) for pred in tokenized_predictions)\n",
        "    reference_tokens = sum(len(ref) for ref in tokenized_references)\n",
        "\n",
        "    precision = true_positives / predicted_tokens if predicted_tokens > 0 else 0\n",
        "    recall = true_positives / reference_tokens if reference_tokens > 0 else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if precision + recall > 0 else 0\n",
        "\n",
        "    # Calculate Exact Match\n",
        "    exact_matches = [1 if pred == ref else 0 for pred, ref in zip(predictions, references)]\n",
        "    exact_match_average = sum(exact_matches) / len(references) if references else 0\n",
        "    exact_match_max = max(exact_matches) if exact_matches else 0\n",
        "\n",
        "    return {\n",
        "        \"Exact Match (Average)\": exact_match_average,\n",
        "        \"Exact Match (Max)\": exact_match_max,\n",
        "        \"BLEU\": bleu,\n",
        "        \"ROUGE (Average)\": rouge_averages,\n",
        "        \"ROUGE (Max)\": rouge_max,\n",
        "        \"BERT Precision (Average)\": bert_precision_mean,\n",
        "        \"BERT Recall (Average)\": bert_recall_mean,\n",
        "        \"BERT F1 (Average)\": bert_f1_mean,\n",
        "        \"BERT Precision (Max)\": bert_precision_max,\n",
        "        \"BERT Recall (Max)\": bert_recall_max,\n",
        "        \"BERT F1 (Max)\": bert_f1_max,\n",
        "        \"Precision (Average)\": precision,\n",
        "        \"Recall (Average)\": recall,\n",
        "        \"F1 (Average)\": f1,\n",
        "    }\n",
        "\n",
        "# Example usage\n",
        "metrics = calculate_metrics(predictions, references)\n",
        "\n",
        "# Display results\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric}: {value}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Owjw2iTVQSUL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
